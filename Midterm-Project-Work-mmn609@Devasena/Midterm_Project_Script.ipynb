{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDTERM PROJECT SCRIPT FOR MULTI-CLASS CLASSIFICATION USING FOUR SUPERVISED LEARNING MODELS FROM SCIKIT-LEARN NAMELY PERCEPTRON, LOGISTIC REGRESSION, SVM AND KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import csv\n",
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the generated dataset using script *dataGen_Script.py* from *midterm_data.csv* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading into a dataframe df\n",
    "df = pd.read_csv('midterm_data.csv')\n",
    "print(df.tail())\n",
    "print(\"Dimensions of data: \",df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the dataset is labelled and has 300 samples and 3 columns of which 2 are the features X1 and X2 columns and the last one is the class label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting all the samples from the above dataframe *df* and the 2 features X1 and X2 (or column names) into X and class labels (third column) into y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:, [0,1]].values\n",
    "y = df.iloc[:,2].values\n",
    "print(y)\n",
    "\n",
    "#Check the unique class labels to prepare for classification\n",
    "print('Class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the samples belong to the class labels that are of 3 types or 3 numbers: 1, 2 and 3. X variable has X1 and X2 columns, y variable has class labels for all the 300 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random state is for initial shuffling of training dataset after each epoch\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitted data into 70% training and 30% test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling for Optimal Performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardizing the features\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "#Stack arrays so as to make a single array vertically and horizontally\n",
    "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
    "y_combined = np.hstack((y_train, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting function for decision regions in the Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')\n",
    "\n",
    "    # highlight test samples\n",
    "    if test_idx:\n",
    "        # plot all samples\n",
    "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        plt.scatter(X_test[:, 0],\n",
    "                    X_test[:, 1],\n",
    "                    c='',\n",
    "                    edgecolor='black',\n",
    "                    alpha=1.0,\n",
    "                    linewidth=1,\n",
    "                    marker='o',\n",
    "                    s=100, \n",
    "                    label='test set')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model I --> Perceptron: Training a perceptron using Scikit-learn and Plotting with Decision Regions and Classified Data by specifying the indices of samples (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multi-class classification using Scikit-learn perceptron\n",
    "ppn = Perceptron(max_iter=50, eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print(y_pred)\n",
    "print(\"Weights assigned to the features: \",ppn.coef_)\n",
    "print('Number of Misclassified samples: %d' % (y_test != y_pred).sum())\n",
    "#print('Model Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "print('Model Accuracy: %.2f' % ppn.score(X_test_std, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained the perceptron model using fit method. We could see that the misclassified samples is just 1 and we get the accuracy of 99%. This is an evaluation parameter for the model. I used the same parameter for comparing how the four models perform on our dataset later in this Notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the perceptron model using the standardized training data. 30% of test data i.e. last 90 samples are the test data i.e. from 210 to 300 samples. Plot with classification and decision regions is stored in a file called *ppn_classify_plot.png*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X=X_combined_std, y=y_combined,\n",
    "                      classifier=ppn, test_idx=range(210, 300))\n",
    "plt.xlabel('X1 [standardized]')\n",
    "plt.ylabel('X2 [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('ppn_classify_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model II --> Logistic Regression: Training a Logistic Regression model using Scikit-learn and Plotting with Decision Regions and Classified Data by specifying the indices of samples (test data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=100.0, random_state=1)\n",
    "lr.fit(X_train_std, y_train)\n",
    "y_pred_lr = lr.predict(X_test_std)\n",
    "print('Number of Misclassified samples: %d' % (y_test != y_pred_lr).sum())\n",
    "#print('Accuracy: %.2f' % accuracy_score(y_test, y_pred_lr))\n",
    "print('Accuracy: %.2f' % lr.score(X_test_std, y_test))\n",
    "plot_decision_regions(X_combined_std, y_combined,\n",
    "                      classifier=lr, test_idx=range(210, 300))\n",
    "plt.xlabel('X1 [standardized]')\n",
    "plt.ylabel('X2 [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lr_classify_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model III --> Support Vector Machines (SVM): Training a SVM model using Scikit-learn and Plotting with Decision Regions and Classified Data by specifying the indices of samples (test data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training SVM model using Scikit-learn and linear kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With linear kernel\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
    "svm.fit(X_train_std, y_train)\n",
    "y_pred_svm = svm.predict(X_test_std)\n",
    "print('Number of Misclassified samples: %d' % (y_test != y_pred_svm).sum())\n",
    "#print('Accuracy: %.2f' % accuracy_score(y_test, y_pred_svm))\n",
    "print('Accuracy: %.2f' % svm.score(X_test_std, y_test))\n",
    "\n",
    "plot_decision_regions(X_combined_std, \n",
    "                      y_combined,\n",
    "                      classifier=svm, \n",
    "                      test_idx=range(210, 300))\n",
    "plt.xlabel('X1 [standardized]')\n",
    "plt.ylabel('X2 [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('svmlin_classify_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training SVM model using Scikit-learn and non-linear kernel called Radial Basis Function (RBF) for softer decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_nonlinear = SVC(kernel='rbf',random_state=0,gamma=0.2, C=1.0)\n",
    "svm_nonlinear.fit(X_train_std, y_train)\n",
    "y_pred_svm_nl = svm_nonlinear.predict(X_test_std)\n",
    "print('Number of Misclassified samples: %d' % (y_test != y_pred_svm_nl).sum())\n",
    "#print('Accuracy: %.2f' % accuracy_score(y_test, y_pred_svm_nl))\n",
    "print('Accuracy: %.2f' % svm_nonlinear.score(X_test_std, y_test))\n",
    "plot_decision_regions(X_combined_std, \n",
    "                      y_combined,\n",
    "                      classifier=svm_nonlinear, \n",
    "                      test_idx=range(210, 300))\n",
    "plt.xlabel('X1 [standardized]')\n",
    "plt.ylabel('X2 [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('svmnon_lin_classify_plot.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model IV --> K-Nearest Neighbours (KNN): Training a KNN model using Scikit-learn and Plotting with Decision Regions and Classified Data by specifying the indices of samples (test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with p=2 i.e. Euclidean distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           p=2, \n",
    "                           metric='minkowski')\n",
    "knn.fit(X_train_std, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_std)\n",
    "print('Number of Misclassified samples: %d' % (y_test != y_pred_knn).sum())\n",
    "#print('Accuracy: %.2f' % accuracy_score(y_test, y_pred_knn))\n",
    "print('Accuracy: %.2f' % knn.score(X_test_std, y_test))\n",
    "\n",
    "plot_decision_regions(X_combined_std, y_combined, \n",
    "                      classifier=knn, test_idx=range(210, 300))\n",
    "\n",
    "plt.xlabel('X1 [standardized]')\n",
    "plt.ylabel('X2 [standardized]')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('knn_classify_plot.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the models with misclassified samples and accuracy (in %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table below lists the four trained models on our dataset with number of misclassified samples and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model | Misclassifications  |Accuracy|\n",
    "|------|------|---------------|--------|\n",
    "| Perceptron | 1 | 99% |\n",
    "| Logistic Regression | 1 | 99% |\n",
    "| SVM | 0 | 100% |\n",
    "| KNN | 0 | 100% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the four models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script and plot showing how the four models (perceptron, logistic regression, svm linear and non-linear and knn) perform on our *midterm_data.csv* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proportion of samples\n",
    "heldout = [0.95, 0.90, 0.75, 0.50, 0.01]\n",
    "rounds = 20\n",
    "\n",
    "#Classifiers (supervised learning models used in this analysis) as a list and initialized\n",
    "classifiers = [\n",
    "    (\"Perceptron\", Perceptron(max_iter=50, eta0=0.1, random_state=1)),\n",
    "    (\"LR\", LogisticRegression(C=100.0, random_state=1)),\n",
    "    (\"SVM_Linear\",SVC(kernel='linear', C=1.0, random_state=1)),\n",
    "    (\"SVM_RBF\",SVC(kernel='rbf',random_state=0,gamma=0.2, C=1.0)),\n",
    "    (\"KNN\",KNeighborsClassifier(n_neighbors=5, \n",
    "                           p=2, \n",
    "                           metric='minkowski'))\n",
    "]\n",
    "\n",
    "#Training sample size proportion\n",
    "xx = 1. - np.array(heldout)\n",
    "\n",
    "#Computing the score below by training the models and predicting the class labels and classifying them by looping through the list of classifiers declared above\n",
    "for name, clf in classifiers:\n",
    "    print(\"training %s\" % name)\n",
    "    rng = np.random.RandomState(42)\n",
    "    yy = []\n",
    "    for i in heldout:\n",
    "        yy_ = []\n",
    "        for r in range(rounds):\n",
    "            X_train, X_test, y_train, y_test = \\\n",
    "                train_test_split(X, y, test_size=i, random_state=rng)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            yy_.append(1 - np.mean(y_pred == y_test))\n",
    "        yy.append(np.mean(yy_))\n",
    "    plt.plot(xx, yy, label=name)\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Training data proportion\")\n",
    "plt.ylabel(\"Test Error Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis we see that the test error rate for **perceptron** model initially shows few misclassifications but reduces it as the training data proportion increases. Whereas the **Logistic Regression** model almost stays constant at low test error rate for any training set proportion at 0.02. **SVM Linear** and **SVM RBF** have 0 test error rate proving to be the best models for our current dataset. **KNN** model follows the same trend as **Logistic Regression** model but starting with higher test error rate and sudden reduction to 0 test error rate with the increase in training dataset. This shows and conforms to the fact that KNN remembers the dataset from the point it stops misclassifying. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the classification plot of the four models with decision regions, we see that except **Perceptron** and **Logistic Regression**, **SVM** and **KNN** show the clear classification of samples based on their class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
